---
layout: post
title: "Encora Academy: Week #4"
---

This has been the fourth and last week of the reset phase of Encora Academy. These are the materials that we covered this past week:

I watched this Matthew Syed's [talk](https://www.youtube.com/watch?v=MmVCYqs3mko) where he talks about “Black Box Thinking” as a way to develop a growth mindset. We had already read and watched some stuff about growth mindset but this time, the speaker tries to do some social-historical analysis with this concept. Groups of people have cultures and these cultures can involve a growth mindset or a fixed mindset. He proposes that cultures with a fixed mindset reproduce shame as punishment for mistakes, therefore this prevents people from accepting them so they make excuses and justify themselves, there can hardly be any kind of progress in this environment; an example of this is the medical field. In contrast, cultures with growth mindset encourage people to learn from mistakes and it is not only an individual process, but a collective one; we can see this in the aviation industry with the implementation of black boxes and this has had a huge impact since it has drastically reduced accidents. 

This talk was valuable for me since I had been seeing growth mindset more as an individual-personal thing but I wasn't considering its collective dimension and the consequences that it can have. For instance, I think that it is so important to consider it if we are involved in some sort of collective/community organizing effort: if we just shame people for making mistakes, they will eventually leave the initiative and other people won't feel excited to join and support; in addition, it is less likely that we would get closer to our collective goal if we are not making some progress from our collective shortcomings. 

Moreover, we watched some more talks about science and computation. I learned about Stephen Wolfram from his 2010 [TED](https://www.youtube.com/watch?v=60P7717-XOQ) talk and his quest to find a unique simple rule for the universe. He proposes that it must be abstract and very low level, in the form of a network, since universe models with very simple rules can successfully reproduce relativity and gravitation. I am glad that I watched this since for me, Wolfram Alpha was this cool application where I could check my calculus homework results. It is amazing to see that it is capable of sooo much more. Last year, ten years after this talk, Wolfram takes a new step towards his goal by launching the [Wolfram Physics Project](https://theconversation.com/a-new-kind-of-physics-stephen-wolfram-has-a-radical-plan-to-build-the-universe-from-dots-and-lines-136830). I think that it is so interesting that it aims to be open sourced. 

I really liked Seth Lloyd's perspective from this [talk](https://www.youtube.com/watch?v=I47TcQmYyo4): since the big bang, the energy that was generated exists in the form of quantum bits, so the universe itself is constantly computing quantum information, so quantum computing is just about 'talking' to these particles to know what they 'say' about themselves, somehow.

Another cool aspect of quantum computing that Seth Lloyd addresses in another  [talk](https://www.youtube.com/watch?v=wkBPp9UovVU) is that it suits machine learning really well, specially when it comes to huge datasets. As we know, the theoretical basis for machine learning is linear algebra and its implementation is mostly about matrix operations. Qbits are suitable for mapping big data into quantum data and the superposition makes all these operations much more efficient. When I saw that the algorithms that might speed up the most due to quantum computing include the [Fourier transform](https://www.cs.bham.ac.uk/internal/courses/intro-mqc/current/lecture06_handout.pdf), MonteCarlo and matrix inversion, I immediately thought of my geophysics college classes, since these algorithms are quite common for diverse data processing steps. I supposed that seismology might benefit a lot from this tool, since the wave propagation modelling and inversion operations require a lot of computational resources. It looks like the exploration industry is [already considering this](https://www.crewes.org/ForOurSponsors/ConferenceAbstracts/2018/SEG/Moradi_SEG_2018.pdf), specially for oil exploration (which is not surprising because that industry is the one that can afford the latest technology advances, a bit sad if you ask me).

Another key figure in the development of physics and computer science (well, science in general) is Richard Feynman. What was more peculiar about him in comparison with other remarkable scientists was his personality that made him a great science communicator and friend, as Danny Hillis remembers in this moving [talk](https://www.youtube.com/watch?v=8CKW4A6jnJA). He was not afraid to fail, for example. As we can see in [these](https://www.youtube.com/watch?v=JIJw3OLB9sI) two [talks](https://www.youtube.com/watch?v=9miKIWIYi4w), his main contributions to science involve a theory about Quantum Electron Dynamics, pipelining (a parallel computing technique), and valuable perspectives of computation regarding efficiency and energy consumption.

[Feynman](https://www.youtube.com/watch?v=EYPapE-3FRw) describes the scientific method in a very clever and realistic way.  There is this misunderstanding of science as a way of finding The Truth and absolute certainties. But this can't be further from reality. As Feynman explains, the process usually involves making a guess first and then compute its consequences and comparing the experiment to the initial guess. This process only allows us to say if the guess has been proven wrong or not (and not being proven wrong is not the same as being proven right). We can never be completely sure we are right, only sure when we are wrong.

We can incorporate the previous scientific method approach to software development. Alberto Savoia's Pretotyping Manifesto is one example of this. He proposes a series of experiments for validating the market appeal and actual usage of a potential new product by simulating its core experience with the smallest possible investment of time and money. He challenges some assumptions regarding ideas viability, for example what he calls the law of failure: most new ideas fail, even if they are well executed so it is important to make sure that you are building the right 'it' before you build 'it' right (and thus invest a lot in doing so). Pretotyping allows you to try more ideas, get faster failures and sometimes more successes.

The last big topic that we covered this week was testing. Google is a company that provides tons on info about how they implement it. They handle two main testing types: automated (bots) and manual (when test cases that haven't been automated yet (e.g. technical debt) or in the case of new features that don't have end to end tests yet). They have 2 different testing-related roles: software engineer in test (focused on development and release efficiency, their customers are developers inside the company) and test engineer (end-user focused, functional testing and user-scenario testing) as described by Ivan Ho & Lindsay Pasricha at Google Tech [Talk](https://www.youtube.com/watch?v=p9bEc6oC6vw) 2014. Alex Eagle mentions at his Google Tech [Talks](https://www.youtube.com/watch?v=J7c0Bw840X8) 2014 that you need to incorporate testing tools that engineers actually want to use, it is also convenient to reduce the cost of testing so you can make your case for it. 

Furthermore, at Google there are some requirements for code reviews like doing them before merging and considering test coverage (how much of the code did the test run), Andrei Chirila [talks](https://www.youtube.com/watch?v=4bublRBCLVQ) about this.

Another test-related element that is important at Google (and described by John Micco at [Google Tech Talk](https://www.youtube.com/watch?v=KH2_sB1A6lA) 2012) is *presubmit*, this process executes all of the tests which are known to be affected by within the code changes that are proposed by a user, this reduces the investment in computing resources.

There is a new approach to testing that is called *Chaos Engineering* (CE), it was created to address failures that happen within the increasing complexity of web services (due to the rise of microservices and cloud architecture), since they are not so predictable and are enormously costly for companies. CE is literally about breaking things (like causing latency, CPU failure, or network black holes) on purpose in order to learn how to build more resilient systems. You can think of it as a vaccine for technical systems so you inject some controlled harm to build immunity by mitigating potential weaknesses. CE also follows the scientific method: first, you plan an experiment and create a hypothesis (in this case: what could go wrong?), secondly you execute the smallest test that will teach you something and if you find an issue, there you are... otherwise, increase the scope of the experiment's affectations until you find an issue or get to full scale. You can find more exciting details about CE in Tammy Butow's [text](https://www.gremlin.com/community/tutorials/chaos-engineering-the-history-principles-and-practice/).

Netflix has been the pioneer of CE, they came up with it since each day they get more and more consumers and depend on Amazon Web Services. As Gareth Bowles (Netflix) [says](https://www.youtube.com/watch?v=xkP70Zhhix4), it was fundamental to find a way to test their systems, provided that it is "so complex that not a single person actually knows how it works". Netflix now implements the FIT framework (Failure Injection Testing), invented by Kolton Andrus (he talks about it during this [interview](https://www.infoq.com/interviews/kolton-andrus-on-breaking-things-at-netflix/) at QCon 2015) so it automates failure injections and the user has the opportunity to decide when and where the failure should occur.

This week we also had to solve a task that relates to pretotyping (you can find the respective report [here](https://github.com/3r3n-n/3r3n-n.github.io/blob/main/otherfiles/3_tested_ideas.pdf) ). It was about testing the viability of 3 ideas so we should run experiments to find this out. What I have learned about this so far is that you need to design the experiments in such a way that people can do the minimal effort to participate in them, it must feel 'natural' and I had not dimensioned how difficult it can be to achieve. 

I have been thinking about some more things this week, basically they relate to how productive I should be during weekdays in order to have actual weekends (LOL) but I don't have a conclusion yet, I think it is just to keep trying until I find something that works for me. But at the same time, I don't know if that is a realistic expectation right now, provided that I still have a couple of other things to do apart from the academy and daily-normal-tasks, at least for the following month. So I guess that the first thing is to be compassionate with myself instead of beating myself up (as I tend to do) because that does not help and just creates unnecessary anxiety. So far, I know that the following things certainly help and I must keep doing them: asking for help soon enough, taking short naps when I feel like headache is about to kick in, and starting (when possible) the most difficult tasks in the morning (when my focus capacity is at its best). Let's see how it goes...
